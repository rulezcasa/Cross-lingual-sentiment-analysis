{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b4ae52",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "cd0399b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from kkltk.kin_kir_stopwords import stopwords \n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.vocab as vocab\n",
    "import spacy\n",
    "from torchtext import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f0284",
   "metadata": {},
   "source": [
    "## Stopwords set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "122964f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset_kin = stopwords.words('kinyarwanda') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928511b9",
   "metadata": {},
   "source": [
    "## Dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3af995bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/train/multilingual_train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69c9a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7982617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tweet = data.tweet.str.replace('[^A-Za-z\\s\\â€™\\-]+', '')\n",
    "data.tweet = data.tweet.str.replace('[\\n]+', '')\n",
    "data.tweet = data.tweet.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "44881bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords\n",
    "data['tweet'] = data['tweet'].apply(lambda x: ' '.join([item.lower() for item in str(x).split() if item not in stopset_kin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "69c9e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/casarulez/Projects/DDSI/SubtaskB/train/multilingual_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789238d",
   "metadata": {},
   "source": [
    "## Importing raw dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7221276f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if i dey enter your eye or you like me and fit...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user ndi igbo is ara di udi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨ÙŠÙˆÙƒÙˆÙ… Ø¹Ù†Ø³ØªÙÙƒÙˆÙ… Ù…Ù† ÙÙˆÙ‚ Ø´ÙˆØ§ÙŠØ© Ø§Ù„Ø§ Ø¨Ù†ÙƒÙŠØ±Ø§...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samia atoa angalizo kuikabili saratani makamu ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. dÃ¹nÃ¹ndÃºn, á»Ì€já»Ì€já»Ì€, ____ áº¹Ì€bÃ , ____, Ã mÃ lÃ  ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63680</th>\n",
       "      <td>á‰µá‹Šá‰°áˆ­ áŠ¥áŠ•á‹° á‹›áˆ¬ á‹°á‰£áˆª áˆ†áŠ– áŠ á‹«á‹á‰…áˆá¢áˆ…á‹á‰¤ á‹ˆáŒ¥áˆ® áŒ­áˆ«á‹áŠ• áŠ¥á‹¨á‰†áˆ‹ áŠá‹á¢</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63681</th>\n",
       "      <td>á»Ì€tÃ kÃ¬tÃ¬ pá»Ìn'wá»Ì lÃ¡, Ã² b'omi 'áº¹Ìnu fáº¹Ì'nÃ¡ jÃ³,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63682</th>\n",
       "      <td>ØºÙŠØ± Ø§Ù†Ø§ Ø§Ù„ÙŠ Ù…Ø¬Ø§Ù†ÙŠØ´ Ø¯Ø¹Ù… Ù…Ù† Ù‡Ø§Ø°Ø§ Ø§Ù„ØªÙŠÙ… ÙˆØ§Ù‚ÙŠÙ„ ğŸ’” @...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63683</th>\n",
       "      <td>áŠ¨áŠ á‹µáŠ•áŠ³áŠ‘ áŠá‹‹ áˆˆá‹›á‹áˆ ???â™‚ï¸?????? á‹¨áˆ áˆ­áŒ á‹­áˆáŠ• á‹¨áˆˆá‰…áˆ¶ áŠ¥áˆ± á‹­á‹ˆá‰€á‹</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63684</th>\n",
       "      <td>mo kÃ­ yÃ­n nÃ­ Ã¬lÃº Ã¨kÃ³ o. Ã¨kÃ³ akÃ©te Ã¬lÃº Ã tÃ bÃ tÃºb...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63685 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet     label\n",
       "0      if i dey enter your eye or you like me and fit...  positive\n",
       "1                     @user @user ndi igbo is ara di udi  negative\n",
       "2      ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨ÙŠÙˆÙƒÙˆÙ… Ø¹Ù†Ø³ØªÙÙƒÙˆÙ… Ù…Ù† ÙÙˆÙ‚ Ø´ÙˆØ§ÙŠØ© Ø§Ù„Ø§ Ø¨Ù†ÙƒÙŠØ±Ø§...  negative\n",
       "3      samia atoa angalizo kuikabili saratani makamu ...   neutral\n",
       "4      5. dÃ¹nÃ¹ndÃºn, á»Ì€já»Ì€já»Ì€, ____ áº¹Ì€bÃ , ____, Ã mÃ lÃ  ...   neutral\n",
       "...                                                  ...       ...\n",
       "63680     á‰µá‹Šá‰°áˆ­ áŠ¥áŠ•á‹° á‹›áˆ¬ á‹°á‰£áˆª áˆ†áŠ– áŠ á‹«á‹á‰…áˆá¢áˆ…á‹á‰¤ á‹ˆáŒ¥áˆ® áŒ­áˆ«á‹áŠ• áŠ¥á‹¨á‰†áˆ‹ áŠá‹á¢  negative\n",
       "63681  á»Ì€tÃ kÃ¬tÃ¬ pá»Ìn'wá»Ì lÃ¡, Ã² b'omi 'áº¹Ìnu fáº¹Ì'nÃ¡ jÃ³,...  positive\n",
       "63682  ØºÙŠØ± Ø§Ù†Ø§ Ø§Ù„ÙŠ Ù…Ø¬Ø§Ù†ÙŠØ´ Ø¯Ø¹Ù… Ù…Ù† Ù‡Ø§Ø°Ø§ Ø§Ù„ØªÙŠÙ… ÙˆØ§Ù‚ÙŠÙ„ ğŸ’” @...  negative\n",
       "63683  áŠ¨áŠ á‹µáŠ•áŠ³áŠ‘ áŠá‹‹ áˆˆá‹›á‹áˆ ???â™‚ï¸?????? á‹¨áˆ áˆ­áŒ á‹­áˆáŠ• á‹¨áˆˆá‰…áˆ¶ áŠ¥áˆ± á‹­á‹ˆá‰€á‹   neutral\n",
       "63684  mo kÃ­ yÃ­n nÃ­ Ã¬lÃº Ã¨kÃ³ o. Ã¨kÃ³ akÃ©te Ã¬lÃº Ã tÃ bÃ tÃºb...  positive\n",
       "\n",
       "[63685 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/train/multilingual_train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051156be",
   "metadata": {},
   "source": [
    "## Cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4d121882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(csv_input_path, csv_output_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(csv_input_path)\n",
    "    \n",
    "    # Ensure all entries in the 'tweet' column are strings\n",
    "    data['tweet'] = data['tweet'].astype(str)\n",
    "    \n",
    "    # Remove symbols but keep letters and spaces\n",
    "    data.tweet = data.tweet.str.replace(r'[^\\w\\s\\â€™\\-]+', '', regex=True)\n",
    "\n",
    "    # Remove newline characters\n",
    "    data.tweet = data.tweet.str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "    # Remove URLs\n",
    "    data.tweet = data.tweet.str.replace(r'http\\S+|www\\S+', '', regex=True)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    data.tweet = data.tweet.apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "\n",
    "    # Save the cleaned data to a new CSV file\n",
    "    data.to_csv(csv_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7873c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets('/Users/casarulez/Projects/DDSI/SubtaskB/train/multilingual_train.csv','/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c0c4959f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tweets('/Users/casarulez/Projects/DDSI/SubtaskB/test/multilingual_test.csv','/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c907c66",
   "metadata": {},
   "source": [
    "## Generating word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "23938df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the data\n",
    "data_train = pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_train.csv')\n",
    "data_test = pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_test.csv')\n",
    "\n",
    "# Concatenate the data\n",
    "data = pd.concat([data_train, data_test])\n",
    "\n",
    "# Ensure 'tweet' and 'label' columns do not have missing values\n",
    "data['tweet'] = data['tweet'].fillna('')\n",
    "data['label'] = data['label'].fillna('')\n",
    "\n",
    "# Create a combined 'whole_doc' column\n",
    "data['whole_doc'] = data['tweet'] + ' ' + data['label'].astype(str)\n",
    "\n",
    "# Prepare the sentences for Word2Vec\n",
    "sentences = [row.split() for row in data['whole_doc'] if len(row)]\n",
    "sentences = [[token.lower() for token in sentence if token] for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, sg=1, hs=1, vector_size=50)\n",
    "\n",
    "# Save the word vectors\n",
    "with open(\"/Users/casarulez/Projects/DDSI/SubtaskB/trained_embeddings/embeddings.txt\", 'w') as output:\n",
    "    for token in w2v_model.wv.key_to_index:\n",
    "        vector_str = ' '.join(map(str, w2v_model.wv[token]))\n",
    "        output.write(f\"{token} {vector_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c7efc",
   "metadata": {},
   "source": [
    "## Loading the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "130aba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the custom embeddings\n",
    "custom_embeddings = vocab.Vectors(name='/Users/casarulez/Projects/DDSI/SubtaskB/trained_embeddings/embeddings.txt',\n",
    "                                  cache='/Users/casarulez/Projects/DDSI/SubtaskB/trained_embeddings/',\n",
    "                                  unk_init=torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "09089cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "15e0c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext import data\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define fields to hold the data\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "TWEET = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ac6e8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label', LABEL), ('tweet', TWEET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49561e02",
   "metadata": {},
   "source": [
    "## Loading cleaned train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "64415380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_train.csv')\n",
    "test_data=pd.read_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b410d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63685 entries, 0 to 63684\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   63685 non-null  object\n",
      " 1   label   63685 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 995.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b8353181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30210 entries, 0 to 30209\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   30210 non-null  object\n",
      " 1   label   30210 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 472.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013c8d4",
   "metadata": {},
   "source": [
    "### Dropping null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c629317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2629e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63685 entries, 0 to 63684\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   63685 non-null  object\n",
      " 1   label   63685 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 995.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c53b401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cb8464e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30210 entries, 0 to 30210\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   30210 non-null  object\n",
      " 1   label   30210 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 708.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3408ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "62b3f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "205da6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(\n",
    "    path='/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_train.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d5e3408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(\n",
    "    path='/Users/casarulez/Projects/DDSI/SubtaskB/cleaned/cleaned_test.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30aa408",
   "metadata": {},
   "source": [
    "## Building vocabulary objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9d3d43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "TWEET.build_vocab(train_data.tweet, max_size=15000, vectors=custom_embeddings)\n",
    "LABEL.build_vocab(train_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8787fd",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f1fb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.content),\n",
    "    device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0bb85142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                          bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3ebe0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instance of the model\n",
    "INPUT_DIM = len(TWEET.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "pretrained_embeddings = TWEET.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9bceb796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(6, 50)\n",
      "  (rnn): GRU(50, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=62616, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bf9a2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # Round predictions to the closest integer\n",
    "    rounded_preds = torch.max(preds, -1)[1]\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e2095ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\" Training the model\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.tweet).squeeze(1)  # Assuming tweet is the text data\n",
    "        loss = criterion(predictions, batch.label.type(torch.cuda.LongTensor))\n",
    "        acc = multiclass_accuracy(predictions, batch.label.type(torch.cuda.LongTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500fd468",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ca4b934e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [232], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loop over epochs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Call the train function and get the loss and accuracy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Print training metrics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% |\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [231], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mtweet)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming tweet is the text data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m acc \u001b[38;5;241m=\u001b[39m multiclass_accuracy(predictions, batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mLongTensor))\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Call the train function and get the loss and accuracy\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "59ec3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6778c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
