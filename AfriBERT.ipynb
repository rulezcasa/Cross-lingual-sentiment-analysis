{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Dependencies and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MWS-Jt0vp3Tl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Iki0tvkOqCZd"
   },
   "outputs": [],
   "source": [
    "kinn_train_path = 'KINNEWS_train.csv'\n",
    "kinn_test_path = 'KINNEWS_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsryH9XMR2eL",
    "outputId": "31dee2fb-ddee-4c80-853e-d6f9e2546d3f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(kinn_train_path)\n",
    "\n",
    "\n",
    "# Concatenating title and content\n",
    "separator = \" [SEP] \"\n",
    "df['text'] = df['title'] + separator + df['content']\n",
    "df = df.drop(columns=['title', 'content'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "COXykb2nUmia",
    "outputId": "ab7b9ffb-3da3-4b1b-e6eb-14dcf11a0a41"
   },
   "outputs": [],
   "source": [
    "# Making the labels 0 based (0 to 13 instead of 1 to 14)\n",
    "df['label'] = df['label'] - 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-val spit\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'].astype(str), df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj0c4uQs_AIW",
    "outputId": "b19ff5a5-c3dc-4888-93ab-0152877ed191",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "\n",
    "# Tokenize the input texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNl_MEFJ_UZX"
   },
   "outputs": [],
   "source": [
    "#Defining the dataset for model training\n",
    "class KinyarwandaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayTWpMxZ_kY2"
   },
   "outputs": [],
   "source": [
    "train_dataset = KinyarwandaDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = KinyarwandaDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"castorini/afriberta_large\", num_labels=14).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/casarulez/Projects/DDSI/AfriBERT/results',          # Output directory\n",
    "    num_train_epochs=25,              # Number of training epochs\n",
    "    per_device_train_batch_size=32,   # Batch size for training\n",
    "    per_device_eval_batch_size=32,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps',\n",
    "    no_cuda=True                     # Disable CUDA\n",
    ")\n",
    "\n",
    "# Function to check dataset shapes\n",
    "def check_dataset_shapes(dataset):\n",
    "    for i, data in enumerate(dataset):\n",
    "        inputs, labels = data\n",
    "        print(f\"Batch {i} - inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
    "        if i == 2:  # Check first few batches\n",
    "            break\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments, defined above\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LmUqZxOUNG2G",
    "outputId": "2dd789c0-aeb3-4df5-ba98-54806893935d"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resuming training from 500 steps checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint='/Users/casarulez/Projects/DDSI/BantuBERT/results/checkpoint-500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhKBab059FNI",
    "outputId": "29e1b6dd-34e5-4850-90d5-731cc05dbeb4"
   },
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv(kinn_test_path)\n",
    "\n",
    "separator = \" [SEP] \"\n",
    "df_test['text'] = df_test['title'] + separator + df_test['content']\n",
    "\n",
    "df_test = df_test.drop(columns=['title', 'content'])\n",
    "df_test['label'] = df_test['label'] - 1\n",
    "\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from the optimum checkpoint and testing metrics - Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "checkpoint_path = '/Users/casarulez/Projects/DDSI/AfriBERT/results/checkpoint-1000'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Set a maximum sequence length that matches the model's expected input size\n",
    "max_length = 512  # You can adjust this based on your model's maximum input length\n",
    "\n",
    "# Tokenize the test data\n",
    "encodings = tokenizer(list(df_test['text']), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "labels = torch.tensor(df_test['label'].values)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        input_ids, attention_mask, label_ids = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(label_ids.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random prediction - Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(df_test), num_samples, replace=False)\n",
    "\n",
    "# Extract texts and labels for these indices\n",
    "sample_texts = df_test.iloc[random_indices]['text'].tolist()\n",
    "sample_labels = df_test.iloc[random_indices]['label'].tolist()\n",
    "\n",
    "# Tokenize the sample texts\n",
    "encodings = tokenizer(list(df_test['text']), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Prepare inputs for the model\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels = torch.tensor(sample_labels)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Print the results\n",
    "for idx, (text, true_label, pred) in enumerate(zip(sample_texts, sample_labels, preds)):\n",
    "    print(f\"Sample {idx + 1}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Kirundi test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "kir_test = pd.read_csv('/Users/casarulez/Projects/DDSI/KIRNEWS/cleaned/test.csv')\n",
    "\n",
    "separator = \" [SEP] \"\n",
    "kir_test['text'] = kir_test['title'] + separator + kir_test['content']\n",
    "\n",
    "kir_test = kir_test.drop(columns=['title', 'content'])\n",
    "kir_test['label'] = kir_test['label'] - 1\n",
    "\n",
    "print(kir_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from the optimum checkpoint and testing metrics - Kirundi (Before fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "checkpoint_path = '/Users/casarulez/Projects/DDSI/AfriBERT/results/checkpoint-1000'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Tokenize the test data\n",
    "max_length=512\n",
    "encodings = tokenizer(list(kir_test['text']), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "labels = torch.tensor(kir_test['label'].values)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        input_ids, attention_mask, label_ids = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(label_ids.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random prediction - Kirundi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(df_test), num_samples, replace=False)\n",
    "\n",
    "# Extract texts and labels for these indices\n",
    "sample_texts = kir_test.iloc[random_indices]['text'].tolist()\n",
    "sample_labels = kir_test.iloc[random_indices]['label'].tolist()\n",
    "\n",
    "# Tokenize the sample texts\n",
    "encodings = tokenizer(sample_texts, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Prepare inputs for the model\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels = torch.tensor(sample_labels)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Print the results\n",
    "for idx, (text, true_label, pred) in enumerate(zip(sample_texts, sample_labels, preds)):\n",
    "    print(f\"Sample {idx + 1}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning on Kirundi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Kirundi training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "kir_train = pd.read_csv('/Users/casarulez/Projects/DDSI/KIRNEWS/cleaned/train.csv')\n",
    "\n",
    "separator = \" [SEP] \"\n",
    "kir_train['text'] = kir_train['title'] + separator + kir_train['content']\n",
    "\n",
    "kir_train = kir_train.drop(columns=['title', 'content'])\n",
    "kir_train['label'] = kir_train['label'] - 1\n",
    "\n",
    "print(kir_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-val spit\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(kir_train['text'].astype(str), kir_train['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "\n",
    "# Tokenize the input texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the dataset for model training\n",
    "class KirundiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KirundiDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = KirundiDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the training parameters\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model = model = AutoModelForSequenceClassification.from_pretrained('/Users/casarulez/Projects/DDSI/AfriBERT/results/checkpoint-1000', num_labels=14).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/casarulez/Projects/DDSI/BantuBERT/results/kirundi',          # Output directory\n",
    "    num_train_epochs=8,              # Number of training epochs\n",
    "    per_device_train_batch_size=32,   # Batch size for training\n",
    "    per_device_eval_batch_size=32,    # Batch size for evaluation\n",
    "    warmup_steps=100,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps',\n",
    "    no_cuda=True                     # Disable CUDA\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments, defined above\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/Users/casarulez/Projects/DDSI/AfriBERT/results/kirundi/checkpoint-500'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Tokenize the test data\n",
    "max_length=512\n",
    "encodings = tokenizer(list(kir_test['text']), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "labels = torch.tensor(kir_test['label'].values)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        input_ids, attention_mask, label_ids = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(label_ids.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing forgetting on kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  ikipe yâ€™ u rwanda amavubi yahesheje u rwanda a...\n",
      "1     10  urubyiruko itorero erc giterane cyâ€™ububyutse k...\n",
      "2      3  rusizi bambaye udupfukamunwa nâ€™ubwo bamwe bata...\n",
      "3      4  abanyarwanda batatu begukanye ibihembo pam awa...\n",
      "4     10  light family choir igiye gukora igitaramo cyâ€™a...\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv(kinn_test_path)\n",
    "\n",
    "separator = \" [SEP] \"\n",
    "df_test['text'] = df_test['title'] + separator + df_test['content']\n",
    "\n",
    "df_test = df_test.drop(columns=['title', 'content'])\n",
    "df_test['label'] = df_test['label'] - 1\n",
    "\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/casarulez/anaconda3/envs/ddsi/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "checkpoint_path = '/Users/casarulez/Projects/DDSI/AfriBERT/results/kirundi/checkpoint-500'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [13:53<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8061\n",
      "F1 Score: 0.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Set a maximum sequence length that matches the model's expected input size\n",
    "max_length = 512  # You can adjust this based on your model's maximum input length\n",
    "\n",
    "# Tokenize the test data\n",
    "encodings = tokenizer(list(df_test['text']), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "labels = torch.tensor(df_test['label'].values)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        input_ids, attention_mask, label_ids = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(label_ids.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
